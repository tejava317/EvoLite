# configs/task_descriptions.yaml
# Task descriptions for each dataset
# These are appended to the FIRST agent's system prompt only

MBPP:
  description: |
    The MBPP (Mostly Basic Python Problems) dataset comprises approximately 1,000 crowd-sourced Python programming problems,
    each including a task description, code solution, and three automated test cases,
    designed to be solvable by entry-level programmers and covering programming fundamentals and standard library functionality.

MATH:
  description: |
    The MATH dataset contains challenging competition mathematics problems requiring multi-step reasoning.
    Each problem requires careful mathematical analysis, and the final answer should be provided in LaTeX boxed notation.

CRUX-O:
  description: |
    The CRUX-O (Code Reasoning and Understanding eXecution - Output) dataset tests code execution prediction.
    Given Python code and input values, predict the exact output the code produces when executed.
    Requires careful tracing through code logic and handling of edge cases.

HumanEval:
  description: |
    The HumanEval dataset, developed by OpenAI, comprises 164 handcrafted programming problems,
    each including a function signature, docstring, body, and multiple unit tests,
    designed to evaluate the code generation capabilities of large language models
    by assessing their ability to generate functionally correct code from docstrings.

LiveCodeBench:
  description: |
    The code generation task in LiveCodeBench involves generating correct and functional code
    from a natural language problem description, where the model is evaluated based on its ability
    to pass a set of unseen test cases. Problems are from recent competitive programming contests.
